{"cells":[{"cell_type":"markdown","metadata":{},"source":["# imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","from transformers import  AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSequenceClassification\n","\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import LabelBinarizer\n","\n","from torcheval.metrics.functional import multiclass_f1_score\n","\n","import random\n","\n","import numpy as np\n","\n","import re\n","\n","from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n","\n","from sklearn import metrics\n","\n","from tqdm.notebook import tqdm_notebook\n","\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","\n","import optuna\n","\n","# variable to use optuna\n","# Turn it to True in order to run the optuna code\n","use_optuna = False"]},{"cell_type":"markdown","metadata":{},"source":["# Load the models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:04:21.583973Z","iopub.status.busy":"2024-02-15T12:04:21.582821Z","iopub.status.idle":"2024-02-15T12:04:25.873418Z","shell.execute_reply":"2024-02-15T12:04:25.872050Z","shell.execute_reply.started":"2024-02-15T12:04:21.583932Z"},"trusted":true},"outputs":[],"source":["tokenizer1 = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\", do_lower_case=True)\n","model1 = AutoModelForSequenceClassification.from_pretrained('nlpaueb/bert-base-greek-uncased-v1', num_labels=3, output_attentions=False, output_hidden_states=False)\n","\n","tokenizer2 = AutoTokenizer.from_pretrained(\"EftychiaKarav/DistilGREEK-BERT\", do_lower_case=True)\n","model2 = AutoModelForSequenceClassification.from_pretrained(\"EftychiaKarav/DistilGREEK-BERT\", num_labels=3, output_attentions=False, output_hidden_states=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Choose Device and Set Seed for Reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:04:25.876183Z","iopub.status.busy":"2024-02-15T12:04:25.875571Z","iopub.status.idle":"2024-02-15T12:04:25.885886Z","shell.execute_reply":"2024-02-15T12:04:25.884434Z","shell.execute_reply.started":"2024-02-15T12:04:25.876151Z"},"trusted":true},"outputs":[],"source":["# Set the seed value \n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Choose device to train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["# Read Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:04:25.888888Z","iopub.status.busy":"2024-02-15T12:04:25.888505Z","iopub.status.idle":"2024-02-15T12:04:26.311734Z","shell.execute_reply":"2024-02-15T12:04:26.310690Z","shell.execute_reply.started":"2024-02-15T12:04:25.888853Z"},"trusted":true},"outputs":[],"source":["# Read the data\n","TrainSet = pd.read_csv('/kaggle/input/ys19-2023-assignment-4a/train_set.csv')\n","ValidationSet = pd.read_csv('/kaggle/input/ys19-2023-assignment-4a/valid_set.csv')\n","TestSet = pd.read_csv('/kaggle/input/ys19-2023-assignment-4a/test_set.csv')\n","\n","# Drop useless columns\n","Xtrain = TrainSet.drop([\"Sentiment\", \"New_ID\"], axis=1)\n","Xval = ValidationSet.drop([\"Sentiment\", \"New_ID\"], axis=1)\n","Xtest = TestSet.drop([\"New_ID\"], axis=1)\n","\n","# Concat Text and Party to Text and Drop Party column\n","Xtrain[\"Text\"] = Xtrain[\"Text\"] + \" \" + Xtrain[\"Party\"]\n","Xval[\"Text\"] = Xval[\"Text\"] + \" \" + Xval[\"Party\"]\n","Xtest[\"Text\"] = Xtest[\"Text\"] + \" \" + Xtest[\"Party\"]\n","\n","# Drop Party column\n","Xtrain = Xtrain.drop([\"Party\"], axis=1)\n","Xval = Xval.drop([\"Party\"], axis=1)\n","Xtest = Xtest.drop([\"Party\"], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["# Word Cloud before Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate the text data from the column 'Text'\n","column_data = ' '.join(Xtrain['Text'])\n","\n","# Create a wordcloud object before cleaning \n","wordcloud = WordCloud(width = 1000, height = 500, background_color='white').generate(column_data)\n","\n","# Plot the wordcloud\n","plt.figure(figsize=(15,7))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Tokens Frequency Diagram before Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create token frequency diagram before cleaning\n","nltk.download('punkt')\n","tokens = [word_tokenize(word, language='greek') for word in Xtrain['Text']]\n","token_frequency = FreqDist(np.hstack(tokens))\n","plt.figure(figsize=(15,7))\n","token_frequency.plot(60, cumulative=False)"]},{"cell_type":"markdown","metadata":{},"source":["# "]},{"cell_type":"markdown","metadata":{},"source":["# Pre-process the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:04:26.313948Z","iopub.status.busy":"2024-02-15T12:04:26.313228Z","iopub.status.idle":"2024-02-15T12:04:26.891834Z","shell.execute_reply":"2024-02-15T12:04:26.889846Z","shell.execute_reply.started":"2024-02-15T12:04:26.313912Z"},"trusted":true},"outputs":[],"source":["# Pre-processing\n","def remove_emoji(string):\n","    emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"\n","                               u\"\\U0001f926-\\U0001f937\"\n","                               u\"\\U00010000-\\U0010ffff\"\n","                               u\"\\u2640-\\u2642\"\n","                               u\"\\u2600-\\u2B55\"\n","                               u\"\\u200d\"\n","                               u\"\\u23cf\"\n","                               u\"\\u23e9\"\n","                               u\"\\u231a\"\n","                               u\"\\ufe0f\"  # dingbats\n","                               u\"\\u3030\"\n","                               \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', string)\n","\n","def remove_links(text):\n","    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","    \n","    # Use the sub() function to replace URLs with an empty string\n","    text_without_links = re.sub(url_pattern, '', text)\n","\n","    return text_without_links\n","\n","# remove emojis\n","Xtrain['Text'] = Xtrain['Text'].map(remove_emoji)\n","Xval['Text'] = Xval['Text'].map(remove_emoji)\n","Xtest['Text'] = Xtest['Text'].map(remove_emoji)\n","\n","# remove links\n","Xtrain['Text'] = Xtrain['Text'].map(remove_links)\n","Xval['Text'] = Xval['Text'].map(remove_links)\n","Xtest['Text'] = Xtest['Text'].map(remove_links)\n","\n","# remove mentions\n","Xtrain['Text'] = Xtrain['Text'].map(lambda x: re.sub(r'@\\S+', '', x))\n","Xval['Text'] = Xval['Text'].map(lambda x: re.sub(r'@\\S+', '', x))\n","Xtest['Text'] = Xtest['Text'].map(lambda x: re.sub(r'@\\S+', '', x))"]},{"cell_type":"markdown","metadata":{},"source":["# Word Cloud after Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate the text data from the column 'Text'\n","column_data = ' '.join(Xtrain['Text'])\n","\n","# Create a wordcloud object after cleaning \n","wordcloud = WordCloud(width = 1000, height = 500, background_color='white').generate(column_data)\n","\n","# Plot the wordcloud\n","plt.figure(figsize=(15,7))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Tokens Frequency Diagram after Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create token frequency diagram after cleaning\n","tokens = [word_tokenize(word, language='greek') for word in Xtrain['Text']]\n","token_frequency = FreqDist(np.hstack(tokens))\n","plt.figure(figsize=(15,7))\n","token_frequency.plot(60, cumulative=False)"]},{"cell_type":"markdown","metadata":{},"source":["#  Encode Categorical Labels Into One-Hot Encoded Vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:04:26.894426Z","iopub.status.busy":"2024-02-15T12:04:26.893422Z","iopub.status.idle":"2024-02-15T12:04:26.907709Z","shell.execute_reply":"2024-02-15T12:04:26.906553Z","shell.execute_reply.started":"2024-02-15T12:04:26.894394Z"},"trusted":true},"outputs":[],"source":["# Get the lists of Sentiments\n","Ytrain = TrainSet.Sentiment.values\n","Yval = ValidationSet.Sentiment.values\n","\n","# Encode the labels\n","encoder = LabelBinarizer()\n","encoder.fit(Ytrain)\n","Ytrain = encoder.transform(Ytrain)\n","Yval = encoder.transform(Yval)\n","\n","# Get the lists of Texts\n","Xtrain = Xtrain.Text.values\n","Xval = Xval.Text.values\n","Xtest = Xtest.Text.values"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenize the Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# First we need to find the maximum length of the sentences\n","def get_max_len(Xtrain, Xval, Xtest, tokenizer):\n","    train_max_len = 0\n","    val_max_len = 0\n","    test_max_len = 0\n","    \n","    for sent in Xtrain:\n","        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","        # Update the maximum sentence length.\n","        train_max_len = max(train_max_len, len(input_ids))\n","\n","    for sent in Xval:\n","        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","        # Update the maximum sentence length.\n","        val_max_len = max(val_max_len, len(input_ids))\n","\n","    for sent in Xtest:\n","        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","        # Update the maximum sentence length.\n","        test_max_len = max(test_max_len, len(input_ids))\n","\n","    # Choose the biggest length\n","    max_len = max(train_max_len, val_max_len, test_max_len)\n","\n","    print('Max sentence length: ', max_len)\n","    return max_len\n","\n","print(\"GreekBERT...\")\n","max_len1 = get_max_len(Xtrain, Xval, Xtest, tokenizer1)\n","print(\"DistilGREEK-BERT...\")\n","max_len2 = get_max_len(Xtrain, Xval, Xtest, tokenizer2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:04:26.909273Z","iopub.status.busy":"2024-02-15T12:04:26.908984Z","iopub.status.idle":"2024-02-15T12:04:42.170355Z","shell.execute_reply":"2024-02-15T12:04:42.169629Z","shell.execute_reply.started":"2024-02-15T12:04:26.909249Z"},"trusted":true},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","def tokenize_and_map(tokenizer, max_len, X, Y = None):\n","    input_ids = []\n","    attention_masks = []\n","\n","    for text in X:\n","        encoded_dict = tokenizer.encode_plus(\n","                            text,                      \n","                            add_special_tokens = True,\n","                            max_length = max_len,  \n","                            truncation=True,\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,   \n","                            return_tensors = 'pt',     \n","                    )  \n","        input_ids.append(encoded_dict['input_ids'])\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors.\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    \n","    if Y is not None:   \n","        Y = torch.tensor(Y,dtype=torch.float32)\n","\n","    return input_ids, attention_masks, Y\n","\n","\n","# For the GreekBERT model\n","Xtrain_ids, Xtrain_attention, Ytrain = tokenize_and_map(tokenizer1, max_len1, Xtrain, Ytrain)\n","Xval_ids, Xval_attention, Yval = tokenize_and_map(tokenizer1, max_len1, Xval, Yval)\n","Xtest_ids, Xtest_attention, _ = tokenize_and_map(tokenizer1, max_len1, Xtest)\n","\n","# For the DistilGREEK-BERT model\n","Xtrain_ids_distil, Xtrain_attention_distil, Ytrain_distil = tokenize_and_map(tokenizer2, max_len2, Xtrain, Ytrain)\n","Xval_ids_distil, Xval_attention_distil, Yval_distil = tokenize_and_map(tokenizer2, max_len2, Xval, Yval)\n","Xtest_ids_distil, Xtest_attention_distil, _ = tokenize_and_map(tokenizer2, max_len2, Xtest)"]},{"cell_type":"markdown","metadata":{},"source":["# Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the ROC curves and the confusion matrices\n","def plot_roc_curve_confusion_matrix(predicted_probabilities, Yval):  \n","    classes_of_interest = [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]\n","    class_ids = [np.flatnonzero(encoder.classes_ == class_of_interest)[0] for class_of_interest in classes_of_interest]\n","    fig, ax = plt.subplots(figsize=(6, 6))\n","\n","    for (i,class_id) in enumerate(class_ids):\n","        RocCurveDisplay.from_predictions(\n","            Yval[:, class_id],\n","            predicted_probabilities.cpu().detach().numpy()[:, class_id],\n","            name=f\"{classes_of_interest[i]} vs the rest\",\n","            color=\"blue\" if class_id == 0 else \"green\" if class_id == 1 else \"red\",\n","            ax = ax\n","        )   \n","    plt.show()\n","    \n","    # Confusion Matrices \n","    predicted_probabilities = torch.argmax(predicted_probabilities, axis=1)\n","\n","    # we convert Yval and Ypred to their initial form\n","    Yval_init = encoder.inverse_transform(Yval)\n","    Ypred_init = np.zeros(shape=(predicted_probabilities.shape[0],3))\n","\n","    Ypred_init[np.arange(len(predicted_probabilities.cpu().detach().numpy())), predicted_probabilities.cpu().detach().numpy()] = 1\n","    Ypred_init = encoder.inverse_transform(Ypred_init)\n","\n","    ConfusionMatrixDisplay.from_predictions(Yval_init, Ypred_init, normalize=\"true\",cmap=plt.cm.YlOrRd)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the learning curves\n","def plot_learning_curves(list_of_train_losses, list_of_valid_losses, list_of_f1_scores_train, list_of_f1_scores_valid):  \n","    # plot validation and training f1 scores \n","    plt.plot(list_of_f1_scores_train)\n","    plt.plot(list_of_f1_scores_valid)\n","    plt.xticks(np.arange(0, len(list_of_f1_scores_train), step=1))\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"F1 scores\",fontweight='bold')\n","    plt.ylim(ymin=0.0, ymax=1.0)\n","    plt.legend([\"Train\", \"Valid\"])\n","    plt.show()\n","\n","    # plot validation and training loss\n","    plt.plot(list_of_train_losses)\n","    plt.plot(list_of_valid_losses)\n","    plt.xticks(np.arange(0, len(list_of_train_losses), step=1))\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Losses\")\n","    plt.legend([\"Train\", \"Valid\"])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:04:42.178587Z","iopub.status.busy":"2024-02-15T12:04:42.178207Z","iopub.status.idle":"2024-02-15T12:04:42.224891Z","shell.execute_reply":"2024-02-15T12:04:42.223041Z","shell.execute_reply.started":"2024-02-15T12:04:42.178565Z"},"trusted":true},"outputs":[],"source":["def train(model, train_dataloader, validation_dataloader, optimizer, scheduler, epochs = 3, first_model = True, loss_func = torch.nn.CrossEntropyLoss):\n","\n","    list_of_train_losses = []\n","    list_of_valid_losses = []\n","    list_of_f1_scores_train = []\n","    list_of_f1_scores_valid = []\n","\n","    # Tell pytorch in which device to train\n","    model = model.to(device)\n","\n","    loss_func = loss_func()\n","\n","    for epoch_i in range(0, epochs):\n","        predicted_probabilities = torch.tensor([], dtype=torch.float32).to(device)\n","        val_predictions = torch.tensor([], dtype=torch.int16, device=device)\n","        val_true_labels = torch.tensor([], dtype=torch.int16, device=device)\n","\n","        train_predictions = torch.tensor([], dtype=torch.int16, device=device)\n","        train_true_labels = torch.tensor([], dtype=torch.int16, device=device)\n","\n","        total_train_loss = 0\n","        total_eval_loss = 0\n","\n","        # Put the model into training mode\n","        model.train()\n","        \n","        for batch in tqdm_notebook(train_dataloader, desc=\"Training: Epoch \" + str(epoch_i + 1)):\n","\n","            # Unpack this training batch from our dataloader.\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            model.zero_grad() \n","\n","            # Perform a forward pass (evaluate the model on this training batch).  \n","            if first_model:\n","                result = model(b_input_ids, \n","                                token_type_ids = None, \n","                                attention_mask = b_input_mask)\n","            else:     \n","                result = model(b_input_ids, \n","                                attention_mask = b_input_mask)\n","            \n","            # The output of the model is a tuple, where the first element is the logits\n","            logits = result.logits\n","\n","            # Compute the loss for this batch.\n","            loss = loss_func(logits, b_labels)\n","            total_train_loss += loss.item()\n","\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","\n","            # Update the learning rate.\n","            scheduler.step()\n","\n","            train_true_labels = torch.cat((train_true_labels, torch.argmax(b_labels, axis=1)))\n","            train_predictions = torch.cat((train_predictions, torch.argmax(logits, axis=1)))\n","            \n","        # Calculate the f1 score for the training set\n","        f1_train = multiclass_f1_score(train_true_labels, train_predictions, average='micro').cpu().tolist()\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  F1 Training Score: {0:.2f}\".format(f1_train))\n","\n","        # Put the model into evaluation mode\n","        model.eval()\n","        \n","        # Evaluate data for one epoch\n","        for batch in tqdm_notebook(validation_dataloader, desc=\"Validation: Epoch \" + str(epoch_i + 1)):\n","            # Unpack this validation batch from our dataloader.\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            with torch.no_grad():  \n","                # Perform a forward pass (evaluate the model on this validation batch).      \n","                if first_model:\n","                    result = model(b_input_ids, \n","                                    token_type_ids = None, \n","                                    attention_mask = b_input_mask)\n","                else:\n","                    result = model(b_input_ids, \n","                                    attention_mask = b_input_mask)\n","            \n","            # The output of the model is a tuple, where the first element is the logits\n","            logits = result.logits\n","\n","            predicted_probabilities = torch.cat((predicted_probabilities, logits))\n","\n","            # Compute the loss for this batch.\n","            loss = loss_func(logits, b_labels)\n","            total_eval_loss += loss.item()\n","\n","            val_predictions = torch.cat((val_predictions, torch.argmax(logits, axis=1)))\n","            val_true_labels = torch.cat((val_true_labels, torch.argmax(b_labels, axis=1)))\n","\n","        # Calculate the f1 score for the validation set\n","        f1_valid = multiclass_f1_score(val_true_labels, val_predictions, average='micro').cpu().tolist()\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  F1 Validatation Score: {0:.2f}\".format(f1_valid))\n","\n","        # Store the loss value for plotting the learning curve\n","        list_of_train_losses.append(avg_train_loss)\n","        list_of_valid_losses.append(avg_val_loss)\n","        list_of_f1_scores_train.append(f1_train)\n","        list_of_f1_scores_valid.append(f1_valid)\n","\n","    # plot the learning curves\n","    plot_learning_curves(list_of_train_losses, list_of_valid_losses, list_of_f1_scores_train, list_of_f1_scores_valid)\n","\n","    return predicted_probabilities, f1_valid"]},{"cell_type":"markdown","metadata":{},"source":["# First Model: GreekBERT Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-15T12:04:42.228088Z","iopub.status.busy":"2024-02-15T12:04:42.227421Z"},"trusted":true},"outputs":[],"source":["# define the batch size\n","batch_size = 32\n","\n","# Create the datasets and dataloaders\n","train_dataset = torch.utils.data.TensorDataset(Xtrain_ids, Xtrain_attention, Ytrain)\n","val_dataset = torch.utils.data.TensorDataset(Xval_ids, Xval_attention, Yval)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, sampler = torch.utils.data.RandomSampler(train_dataset), batch_size = batch_size)\n","validation_dataloader = torch.utils.data.DataLoader(val_dataset, sampler = torch.utils.data.SequentialSampler(val_dataset), batch_size = batch_size)\n","\n","# Define the optimizer \n","optimizer = AdamW(model1.parameters(), lr = 3e-5, eps = 1e-6)\n","\n","# Number of training epochs and the total number of training steps\n","epochs = 2\n","steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = steps)\n","\n","# Variable to check if it is the GreekBERT model or the DistilGREEK-BERT model\n","first_model = True\n","predicted_probabilities, f1_valid = train(model1, train_dataloader, validation_dataloader, optimizer, scheduler, epochs, first_model = first_model)\n","\n","# Plot the ROC curves and the confusion matrices\n","plot_roc_curve_confusion_matrix(predicted_probabilities, Yval)"]},{"cell_type":"markdown","metadata":{},"source":["# Predict the Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict the TestSet\n","with torch.no_grad():\n","    model1.eval()\n","    result = model1(Xtest_ids, token_type_ids = None, attention_mask = Xtest_attention)\n","    Ytest_predict = result.logits\n","    \n","# Convert it to the right form\n","Ytest_predict = torch.argmax(Ytest_predict, axis=1)\n","Ytest_pred_initial = np.zeros(shape=(Ytest_predict.shape[0],3))\n","Ytest_pred_initial[np.arange(len(Ytest_predict.cpu())), Ytest_predict.cpu()] = 1\n","Ytest_pred_initial = encoder.inverse_transform(Ytest_pred_initial)  \n","\n","submission_df = pd.DataFrame({\"Id\": TestSet[\"New_ID\"], \"Predicted\":Ytest_pred_initial})\n","submission_df.to_csv('submission.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Optuna Framework"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Here is implemented the Optuna framework in order to find the best hyperparameters for learning rate, batch size and epochs for the GreekBERT model\n","# Turn the use_optuna variable to True if you want to use it\n","trial_number = -1\n","\n","def optimize_hyperparameters(trial):  \n","    # Define the model again in order to reset the weights\n","    model1 = AutoModelForSequenceClassification.from_pretrained('nlpaueb/bert-base-greek-uncased-v1', num_labels=3, output_attentions=False, output_hidden_states=False)\n","\n","    # suggest the batch size\n","    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n","\n","    # suggest the learning rate\n","    lr = trial.suggest_categorical('lr', [2e-5, 3e-5, 5e-5])\n","\n","    # suggest the number of epochs\n","    epochs = trial.suggest_categorical('epochs', [2, 3, 4])\n","\n","    # Create the datasets and dataloaders\n","    train_dataset = torch.utils.data.TensorDataset(Xtrain_ids, Xtrain_attention, Ytrain)\n","    val_dataset = torch.utils.data.TensorDataset(Xval_ids, Xval_attention, Yval)\n","\n","    train_dataloader = torch.utils.data.DataLoader(train_dataset, sampler = torch.utils.data.RandomSampler(train_dataset), batch_size = batch_size)\n","    validation_dataloader = torch.utils.data.DataLoader(val_dataset, sampler = torch.utils.data.SequentialSampler(val_dataset), batch_size = batch_size)\n","\n","    # Define the optimizer \n","    optimizer = AdamW(model1.parameters(), lr = lr, eps = 1e-6)\n","\n","    steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler.\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = steps)\n","\n","    # Variable to check if it is the GreekBERT model or the DistilGREEK-BERT model\n","    first_model = True\n","    predicted_probabilities, f1_valid = train(model1, train_dataloader, validation_dataloader, optimizer, scheduler, epochs, first_model = first_model)\n","\n","    return f1_valid\n","    \n","# Create the study and optimize the hyperparameters\n","study = None\n","if use_optuna == True: \n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(optimize_hyperparameters, n_trials=15)\n","    optuna.visualization.matplotlib.plot_param_importances(study)\n","    optuna.visualization.matplotlib.plot_slice(study)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Second Model: DistilGREEK-BERT Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define the batch size\n","batch_size = 16\n","\n","# Create the datasets and dataloaders\n","train_dataset = torch.utils.data.TensorDataset(Xtrain_ids_distil, Xtrain_attention_distil, Ytrain_distil)\n","val_dataset = torch.utils.data.TensorDataset(Xval_ids_distil, Xval_attention_distil, Yval_distil)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, sampler = torch.utils.data.RandomSampler(train_dataset), batch_size = batch_size)\n","validation_dataloader = torch.utils.data.DataLoader(val_dataset, sampler = torch.utils.data.SequentialSampler(val_dataset), batch_size = batch_size)\n","\n","# Define the optimizer \n","optimizer = AdamW(model2.parameters(), lr = 5e-5, eps = 1e-6)\n","\n","# Number of training epochs and the total number of training steps\n","epochs = 4\n","steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = steps)\n","\n","# Variable to check if it is the GreekBERT model or the DistilGREEK-BERT model\n","first_model = False\n","predicted_probabilities, f1_valid = train(model2, train_dataloader, validation_dataloader, optimizer, scheduler, epochs, first_model = first_model)\n","\n","# Plot the ROC curves and the confusion matrices\n","plot_roc_curve_confusion_matrix(predicted_probabilities, Yval_distil)"]},{"cell_type":"markdown","metadata":{},"source":["# Predict the Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict the TestSet\n","with torch.no_grad():\n","    model2.eval()\n","    result = model1(Xtest_ids_distil, attention_mask = Xtest_attention_distil)\n","    Ytest_predict = result.logits\n","    \n","# Convert it to the right form\n","Ytest_predict = torch.argmax(Ytest_predict, axis=1)\n","Ytest_pred_initial = np.zeros(shape=(Ytest_predict.shape[0],3))\n","Ytest_pred_initial[np.arange(len(Ytest_predict.cpu())), Ytest_predict.cpu()] = 1\n","Ytest_pred_initial = encoder.inverse_transform(Ytest_pred_initial)  \n","\n","submission_distil_df = pd.DataFrame({\"Id\": TestSet[\"New_ID\"], \"Predicted\":Ytest_pred_initial})\n","submission_distil_df.to_csv('submission_distil.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Optuna Framework"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Here is implemented the Optuna framework in order to find the best hyperparameters for learning rate, batch size and epochs for the DistilGREEK-BERT model\n","# Turn the use_optuna variable to True if you want to use it\n","trial_number = -1\n","\n","def optimize_hyperparameters(trial):  \n","    # Define the model again in order to reset the weights\n","    model2 = AutoModelForSequenceClassification.from_pretrained(\"EftychiaKarav/DistilGREEK-BERT\", num_labels=3, output_attentions=False, output_hidden_states=False)\n","\n","    # suggest the batch size\n","    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n","\n","    # suggest the learning rate\n","    lr = trial.suggest_categorical('lr', [2e-5, 3e-5, 5e-5])\n","\n","    # suggest the number of epochs\n","    epochs = trial.suggest_categorical('epochs', [2, 3, 4])\n","\n","    # Create the datasets and dataloaders\n","    train_dataset = torch.utils.data.TensorDataset(Xtrain_ids_distil, Xtrain_attention_distil, Ytrain_distil)\n","    val_dataset = torch.utils.data.TensorDataset(Xval_ids_distil, Xval_attention_distil, Yval_distil)\n","\n","    train_dataloader = torch.utils.data.DataLoader(train_dataset, sampler = torch.utils.data.RandomSampler(train_dataset), batch_size = batch_size)\n","    validation_dataloader = torch.utils.data.DataLoader(val_dataset, sampler = torch.utils.data.SequentialSampler(val_dataset), batch_size = batch_size)\n","\n","    # Define the optimizer \n","    optimizer = AdamW(model2.parameters(), lr = lr, eps = 1e-6)\n","\n","    steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler.\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = steps)\n","\n","    # Variable to check if it is the GreekBERT model or the DistilGREEK-BERT model\n","    first_model = False\n","    predicted_probabilities, f1_valid = train(model2, train_dataloader, validation_dataloader, optimizer, scheduler, epochs, first_model = first_model)\n","\n","# Create the study object and optimize the hyperparameters\n","study = None\n","if use_optuna == True: \n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(optimize_hyperparameters, n_trials=15)\n","    optuna.visualization.matplotlib.plot_param_importances(study)\n","    optuna.visualization.matplotlib.plot_slice(study)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7632283,"sourceId":68801,"sourceType":"competition"}],"dockerImageVersionId":30648,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
